# SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation 

[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)  
[Project Page](https://syncanimation.github.io/) | [Paper (arXiv)](https://arxiv.org/abs/2501.14646)   
ğŸ“¢ <b>Official repository of SyncAnimation. The paper has been accepted to IJCAI 2025.</b>
<p align="center">
  <img src="assets/image/pipline.png" alt="SyncAnimation Demo" width="1080">
</p>
â€œGenerating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip.â€  

---
## ğŸ§  Introduction  

Most existing audio-driven talking head synthesis methods focus only on the facial region, pasting other parts like the torso from the original image, which leads to audio inconsistency between facial movements, lips, and body motion. **SyncAnimation** addresses this issue by ensuring:  
- **Audio-Body Consistency**  
- **Audio-Face Consistency**  
- **Audio-Lips Consistency** 
 <p align="center">
  <img src="assets/image/objectives.png" alt="SyncAnimation Demo" width="1080">
</p>
---

## ğŸ›  Installation & Dependencies

### Linux / Ubuntu  

The environment setup of this project follows the installation process of [SyncTalk](https://github.com/ZiqiaoPeng/SyncTalk).  Below is the recommended installation process on Ubuntu (tested on Ubuntu 20.04 with PyTorch 1.12.1 + CUDA 11.3):

```bash
git clone https://github.com/syncanimation/syncanimation.git
cd syncanimation

# It is recommended to use a conda environment
conda create -n syncanimation python==3.8.8
conda activate syncanimation

# Install PyTorch and torchvision (choose versions according to your CUDA version)
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113

sudo apt-get install portaudio19-dev
pip install -r requirements.txt

# Install required modules (freqencoder / gridencoder / shencoder / raymarching)
pip install ./freqencoder
pip install ./shencoder
pip install ./gridencoder
pip install ./raymarching

# Install PyTorch3D (if issues occur, use the fallback script)
pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1121/download.html
# Or:
python ./scripts/install_pytorch3d.py

# Install TensorFlow GPU version 
pip install tensorflow-gpu==2.8.1
```

> **Note**ï¼šYou may encounter compatibility issues when installing PyTorch3D. It is recommended to use the scripts/install_pytorch3d.py script as a fallback.

---

<!--
## ğŸ”„ æ•°æ®å‡†å¤‡  

### é¢„è®­ç»ƒæ¨¡å‹  

è¯·å°†ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ `May.zip`ã€`trial_may.zip`ï¼‰æ”¾å…¥ç›¸åº”ç›®å½•ï¼š

- `data/May.zip` â†’ è§£å‹è‡³ `data/May/`  
- `model/trial_may.zip` â†’ è§£å‹è‡³ `model/trial_may/`  

### è¾“å…¥è§†é¢‘å¤„ç†  

1. ä¸‹è½½ face-parsing æ¨¡å‹  
   ```bash
   wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_parsing/79999_iter.pth?raw=true -O data_utils/face_parsing/79999_iter.pth
   ```
2. ä¸‹è½½ 3DMM å¤´å§¿ä¼°è®¡æ¨¡å‹  
   ```bash
   wget â€¦  # å¤šä¸ªæ–‡ä»¶ï¼šexp_info.npy, keys_info.npy, sub_mesh.obj, topology_info.npy  
   ```
3. ä¸‹è½½ Basel Face Model (BFM 2009)ï¼Œè½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼  
   ```bash
   # ä¸‹è½½ .mat æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æ”¾åˆ° data_utils/face_tracking/3DMM/
   cd data_utils/face_tracking
   python convert_BFM.py
   ```
4. è¾“å…¥è§†é¢‘è¦æ±‚ï¼š  
   - å¸§ç‡ 25 FPS  
   - æ¯å¸§åŒ…å«è®²è¯äººé¢éƒ¨  
   - åˆ†è¾¨ç‡æ¨è ~512Ã—512  
   - æ—¶é•¿çº¦ 4â€“5 åˆ†é’Ÿ  

5. æ‰§è¡Œè§†é¢‘å¤„ç†è„šæœ¬  
   ```bash
   python data_utils/process.py data/<ID>/<ID>.mp4 --asr ave
   ```
   - æ”¯æŒ `ave`ã€`deepspeech`ã€`hubert` ä¸‰ç§ç‰¹å¾æå–  
   - å¯é€‰åœ°ï¼Œè¿è¡Œ OpenFace çš„ `FeatureExtraction`ï¼Œç”Ÿæˆçœ¼ç›çœ¨åŠ¨ AU45 ä¿¡æ¯ï¼ˆé‡å‘½åä¸º `data/<ID>/au.csv`ï¼‰  

> æ³¨æ„ï¼šç”±äº EmoTalk çš„ blendshape æ•æ‰æœªå¼€æºï¼Œå› æ­¤è¿™é‡Œé»˜è®¤ä½¿ç”¨ mediapipe çš„ blendshape æ•æ‰ã€‚å¯¹äºæŸäº›åœºæ™¯æ•ˆæœä¸ä½³ï¼Œå¯è€ƒè™‘æ›¿æ¢æˆ–æ”¹è¿›ã€‚  

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹  

### è¯„ä¼° / æ¨ç†  

```bash
python main.py data/May --workspace model/trial_may -O --test --asr_model ave
python main.py data/May --workspace model/trial_may -O --test --asr_model ave --portrait
```

- `--portrait`ï¼šå°†ç”Ÿæˆçš„äººè„¸è´´å›åŸå§‹å›¾åƒ â†’ ç”»è´¨è¾ƒå¥½  
- æˆåŠŸè¿è¡Œåå°†è¾“å‡º PSNR / LPIPS / LMD ç­‰æŒ‡æ ‡  

è‹¥è¦ä½¿ç”¨ç›®æ ‡éŸ³é¢‘è¿›è¡Œæ¨ç†ï¼š

```bash
python main.py data/May --workspace model/trial_may -O --test --test_train --asr_model ave --portrait --aud ./demo/test.wav
```

æ³¨æ„ï¼šéŸ³é¢‘éœ€ä¸º `.wav` æ ¼å¼ã€‚å¦‚æœä½¿ç”¨å…¶ä»–ç‰¹å¾ï¼ˆå¦‚ npyï¼‰ï¼Œå¯æ”¹è·¯å¾„è‡³å¯¹åº”æ–‡ä»¶ã€‚

### è®­ç»ƒ  

é»˜è®¤æ–¹å¼ä»ç£ç›˜æŒ‰éœ€åŠ è½½æ•°æ®ï¼š

```bash
python main.py data/May --workspace model/trial_may -O --iters 60000 --asr_model ave
python main.py data/May --workspace model/trial_may -O --iters 100000 --finetune_lips --patch_size 64 --asr_model ave
# æˆ–è€…ä½¿ç”¨è„šæœ¬
sh ./scripts/train_may.sh
```

è‹¥æƒ³åŠ å…¥èº¯å¹²è®­ç»ƒä»¥ä¿®å¤åŒä¸‹å·´ï¼ˆæ³¨æ„ï¼šè®­ç»ƒèº¯å¹²åä¸èƒ½ç”¨ `--portrait` æ¨¡å¼ï¼‰ï¼š

```bash
python main.py data/May/ --workspace model/trial_may_torso/ -O --torso --head_ckpt <head_ckpt>.pth --iters 150000 --asr_model ave
python main.py data/May --workspace model/trial_may_torso -O --torso --test --asr_model ave
python main.py data/May --workspace model/trial_may_torso -O --torso --test --test_train --asr_model ave --aud ./demo/test.wav
```

---

## ğŸ“Š è¯„ä»·æŒ‡æ ‡  

ç¤ºä¾‹ï¼ˆå•äººï¼‰ï¼š

| æ¨¡å¼                      | PSNR    | LPIPS    | LMD     |
|---------------------------|---------|-----------|---------|
| SyncTalk (ä¸è´´å›åŸå›¾)     | 32.201  | 0.0394    | 2.822   |
| SyncTalk (è´´å›åŸå›¾)       | 37.644  | 0.0117    | 2.825   |

ï¼ˆè®ºæ–‡ç»™å‡ºäº†å¤šä¸ªè¢«è¯•çš„å¹³å‡æŒ‡æ ‡ï¼‰

---
 -->

## ğŸ“ Citation  

Please cite the following paper if you use this method, model, or conduct derivative research based on this project:

```tex
@inproceedings{ijcai2025p185,
  title     = {SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation},
  author    = {Liu, Yujian and Xu, Shidang and Guo, Jing and Wang, Dingbin and Wang, Zairan and Tan, Xianfeng and Liu, Xiaoli},
  booktitle = {Proceedings of the Thirty-Fourth International Joint Conference on
               Artificial Intelligence, {IJCAI-25}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {James Kwok},
  pages     = {1657--1665},
  year      = {2025},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2025/185},
  url       = {https://doi.org/10.24963/ijcai.2025/185},
}
```

---

## ğŸ™ Acknowledgements  

This project is built upon or inspired by the following open-source projects:

- Synctalk  
- ER-NeRF  
- GeneFace   
- AD-NeRF  
- Deep3DFaceRecon_pytorch  

We sincerely thank the authors of these projects for their contributions to the open-source community.

---

## âš ï¸ Disclaimer  

By using this project, you agree to comply with all applicable laws and regulations.
You must not use it to generate or disseminate harmful content.
The developers assume no responsibility for any direct, indirect, or consequential damages arising from the use or misuse of this software. 
